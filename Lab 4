x1_data = [1, 0, 3, 0, 5]
x2_data = [0, 2, 0, 4, 0]
y_data  = [1, 2, 3, 4, 5]
#변수는 x1,x2로 2개이다. y는 예측데이터 label라고 한다.

W1 = tf.Variable(tf.random_uniform([1], -10.0, 10.0))
W2 = tf.Variable(tf.random_uniform([1], -10.0, 10.0))
b  = tf.Variable(tf.random_uniform([1], -10.0, 10.0))
#변수가 2개이므로 weight값도 2개이다. bias는 1개가 필요하다. 
#초기값은 전부 1로 주어진다.

learning_rate = tf.Variable(0.001)
#learning_rate는 작은 상수로 놓는다.

for i in range(1000+1): #1001번 반복한다.
    with tf.GradientTape() as tape: #변수들에 대한 정보를 tape에 입력한다.
        hypothesis = W1 * x1_data + W2 * x2_data + b  
        #가설함수 hyporthesis를 코드로 입력한다.
        cost = tf.reduce_mean(tf.square(hypothesis - y_data))
         #경사하강법은 경사를 하강하면서 cost가 최소화되는 w,b를 찾는 것이다.
    # calculates the gradients of the cost
    W1_grad, W2_grad, b_grad = tape.gradient(cost, [W1, W2, b])
    W1.assign_sub(learning_rate * W1_grad)
    W2.assign_sub(learning_rate * W2_grad)
    b.assign_sub(learning_rate * b_grad)
    #학습은 gradientape으로 구현한다. 
    cost함수는 가설에서 정답데이터를 뺀 오차 제곱의 평균값으로 정한다. 
    변수들의 정보를 tape에 기록. 
    #tape에 gradient를 호출하여 4개의 변수들에 대한 기울기 값을 구한다.
    #3개의 gradient값을 assign_sub을 사용하여 업데이트 한다.

    if i % 50 == 0: #중간중간에 확인을 위해 사용한다.
        print("{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.6f}".format(
          i, cost.numpy(), W1.numpy()[0], W2.numpy()[0], b.numpy()[0]))
          #경사하강을 통해 weight값을 지속적으로 업데이트 해가는 과정이다.
          #i의 값이 30의 배수가 될 때 마다 cost값을 출력한다.
x_data = [
    [1., 0., 3., 0., 5.],
    [0., 2., 0., 4., 0.]
]
y_data  = [1, 2, 3, 4, 5]

W = tf.Variable(tf.random_uniform([1, 2], -1.0, 1.0))
b = tf.Variable(tf.random_uniform([1], -1.0, 1.0))

learning_rate = tf.Variable(0.001)

for i in range(1000+1):
    with tf.GradientTape() as tape:
        hypothesis = tf.matmul(W, x_data) + b # (1, 2) * (2, 5) = (1, 5)
        cost = tf.reduce_mean(tf.square(hypothesis - y_data))

        W_grad, b_grad = tape.gradient(cost, [W, b])
        W.assign_sub(learning_rate * W_grad)
        b.assign_sub(learning_rate * b_grad)
    
    if i % 50 == 0:
        print("{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.6f}".format(
            i, cost.numpy(), W.numpy()[0][0], W.numpy()[0][1], b.numpy()[0]))
            
  import tensorflow as tf

# 앞의 코드에서 bias(b)를 행렬에 추가
x_data = [
    [1., 1., 1., 1., 1.], # bias(b)
    [1., 0., 3., 0., 5.], 
    [0., 2., 0., 4., 0.]
]
y_data  = [1, 2, 3, 4, 5]

W = tf.Variable(tf.random_uniform([1, 3], -1.0, 1.0)) # [1, 3]으로 변경하고, b 삭제

learning_rate = 0.001
optimizer = tf.train.GradientDescentOptimizer(learning_rate)

for i in range(1000+1):
    with tf.GradientTape() as tape:
        hypothesis = tf.matmul(W, x_data) # b가 없다
        cost = tf.reduce_mean(tf.square(hypothesis - y_data))

    grads = tape.gradient(cost, [W])
    optimizer.apply_gradients(grads_and_vars=zip(grads,[W]))
    if i % 50 == 0:
        print("{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.4f}".format(
            i, cost.numpy(), W.numpy()[0][0], W.numpy()[0][1], W.numpy()[0][2]))
            
  # Multi-variable linear regression (1)

X = tf.constant([[1., 2.], 
                 [3., 4.]])
y = tf.constant([[1.5], [3.5]])

W = tf.Variable(tf.random_normal([2, 1]))
b = tf.Variable(tf.random_normal([1]))

# Create an optimizer
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)

n_epoch = 1000+1
print("epoch | cost")
for i in range(n_epoch):
    # Use tf.GradientTape() to record the gradient of the cost function
    with tf.GradientTape() as tape:
        y_pred = tf.matmul(X, W) + b
        cost = tf.reduce_mean(tf.square(y_pred - y))

    # calculates the gradients of the loss
    grads = tape.gradient(cost, [W, b])
    
    # updates parameters (W and b)
    optimizer.apply_gradients(grads_and_vars=zip(grads, [W, b]))
    if i % 50 == 0:
        print("{:5} | {:10.6f}".format(i, cost.numpy()))
        
tf.set_random_seed(0)  # for reproducibility

# data and label
x1 = [ 73.,  93.,  89.,  96.,  73.] #입력1(변수1)
x2 = [ 80.,  88.,  91.,  98.,  66.] #입력2(변수2)
x3 = [ 75.,  93.,  90., 100.,  70.] #입력3(변수3)
Y  = [152., 185., 180., 196., 142.] #출력데이터, 정답, 예측값

# weights
w1 = tf.Variable(10.)
w2 = tf.Variable(10.)
w3 = tf.Variable(10.)
b  = tf.Variable(10.)
#변수가 3개 이므로 W도 3개가 필요하다. b는 하나가 필요하다.
#hypothesis = w1 * x1 + w2 * x2 + w3 * x3 + b

learning_rate = 0.000001

for i in range(1000+1): #1001번 반복한다.
    # tf.GradientTape() to record the gradient of the cost function
    with tf.GradientTape() as tape: #변수들에 대한 정보를 tape로 입력한다.
        hypothesis = w1 * x1 +  w2 * x2 + w3 * x3 + b #가설함수 hyporthesis를 코드로 입력한다.
        cost = tf.reduce_mean(tf.square(hypothesis - Y))
    # calculates the gradients of the cost
    w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1, w2, w3, b])
    #변수([w1,w2,w3,b])들에 대한 기울기의 값을 구한다. 
    #W1_grad는 W1의 기울기,W2_grad는 W2의 기울기,W3_grad는 W3의 기울기, b_grad는 b의 기울기이다.
    
    # update w1,w2,w3 and b
    w1.assign_sub(learning_rate * w1_grad) #learning_rate에서 w1의 기울기 값을 빼서 할당한다.
    w2.assign_sub(learning_rate * w2_grad) #learning_rate에서 w2의 기울기 값을 빼서 할당한다.
    w3.assign_sub(learning_rate * w3_grad) #learning_rate에서 w3의 기울기 값을 빼서 할당한다.
    b.assign_sub(learning_rate * b_grad) #learning_rate에서 b의 기울기 값을 빼서 할당한다.

    if i % 50 == 0:
      print("{:5} | {:12.4f}".format(i, cost.numpy()))
      
# data and label
x1 = [ 73.,  93.,  89.,  96.,  73.] #입력1(변수1)
x2 = [ 80.,  88.,  91.,  98.,  66.] #입력2(변수2)
x3 = [ 75.,  93.,  90., 100.,  70.] #입력3(변수3)
Y  = [152., 185., 180., 196., 142.] #출력데이터, 정답, 예측값

# random weights
w1 = tf.Variable(tf.random_normal([1]))
w2 = tf.Variable(tf.random_normal([1]))
w3 = tf.Variable(tf.random_normal([1]))
b  = tf.Variable(tf.random_normal([1]))
#변수가 3개 이므로 W도 3개가 필요하다. b는 하나가 필요하다.
#랜덤 값을 준다.
learning_rate = 0.000001 #learning_rate는 작은 값으로 준다.

for i in range(1000+1): #1001번 실행한다.
    with tf.GradientTape() as tape: #GradientTap을 이용하여 구현한다.
    #아래 변수들의 정보를 tape에 기록한다.
        hypothesis = w1 * x1 +  w2 * x2 + w3 * x3 + b
        cost = tf.reduce_mean(tf.square(hypothesis - Y))
    w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1, w2, w3, b])
    #tape.gradient에서 이 함수에 대한 4개의 변수([w1, w2, w3, b])들에 대한 기울기 값을 구한다.
    
    # update w1,w2,w3 and b
    w1.assign_sub(learning_rate * w1_grad)
    w2.assign_sub(learning_rate * w2_grad)
    w3.assign_sub(learning_rate * w3_grad)
    #4개의 gradient값을 각각 w1, w2, w3에 업데이트한다.
    #업데이트를 위해 assign_sub를 이용한다.
    b.assign_sub(learning_rate * b_grad)
    #W값을 지속적으로 업데이트하는 내용
    if i % 50 == 0:
      print("{:5} | {:12.4f}".format(i, cost.numpy()))
    #50번 마다 cost값을 출력해 보도록 한다.
      
data = np.array([
    # X1,   X2,    X3,   y
    [ 73.,  80.,  75., 152. ],
    [ 93.,  88.,  93., 185. ],
    [ 89.,  91.,  90., 180. ],
    [ 96.,  98., 100., 196. ],
    [ 73.,  66.,  70., 142. ]
], dtype=np.float32)
#X1, X2, X3이 x_data, y는 y_data이다.

# slice data
X = data[:, :-1] #numpy의 slice를 활용하여 데이터를 잘라낸 것이다.
#5행 3열짜리 매트릭스가 된다. 
y = data[:, [-1]]
# ','를 기준으로 앞부분은 행, 뒷부분은 열을 뜻한다.

W = tf.Variable(tf.random_normal([3, 1])) 
#[3, 1] → x1, x2, x3변수가 3개고 y는 1개이기 때문이다.
b = tf.Variable(tf.random_normal([1]))

learning_rate = 0.000001 
#learning_rate을 작은 상수로 놓는다.

def predict(X): #predict는 x,w로 정의한다.
    return tf.matmul(X, W) + b
    #b는 생략이 가능하다.

n_epochs = 2000 #2000번의 순회를 한다.
for i in range(n_epochs+1): #2001회의 epochs를 수행하도록 한다.
    
    with tf.GradientTape() as tape:
        cost = tf.reduce_mean((tf.square(predict(X) - y)))

    W_grad, b_grad = tape.gradient(cost, [W, b])

    W.assign_sub(learning_rate * W_grad)
    b.assign_sub(learning_rate * b_grad)
    
    if i % 100 == 0:
        print("{:5} | {:10.4f}".format(i, cost.numpy()))
        #최적화가 빠르게 이루어 진다.
