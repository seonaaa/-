import tensorflow.contrib.eager as tfe #tensorflow에서 이 모드로 실행하기 위한 기본적인 라이브러리 import, 라이브러리 된 상태이다.
tf.enable_eager_execution() #tensorflow를 이 모드로 실행하기 위해 execution선언한다.
dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(len(x_train)) 
#가져올 데이터(tf.data)를 통해 원하는 X값과 Y값을 X의 길이 만큼 batch로 학습을 하겠다는 것을 토대로 데이터 값을 가져온다.
#X의 길이 6 원하는 모델 선언 가능하다.
W = tf.Variable(tf.random_normal([2, 1]), name='weight') #모델의 shape가 1행 2열에서 행렬연산을 위해 2행 1열로 바뀌게 된다.
b = tf.Variable(tf.random_normal([1]), name='bias') #W값과 연산을 통해 원하는 값 도출한다.

def logistic_regression(features): #logistic_regression에 대한 hypothesis
    hypothesis  = tf.div(1., 1. + tf.exp(tf.matmul(features, W) + b)) #matmul값을 iogistic_regression을 하기 위해 나온 값 시그노이드 함수
    return hypothesis #위의 값으로 hypothesis를 그려낸다.
def loss_fn(hypothesis, features, labels): #label을 넣는다.
    cost = -tf.reduce_mean(labels * tf.log(logistic_regression(features)) + (1 - labels) * tf.log(1 - hypothesis)) 
    #hypothesis와의 cost값을 구하기 위한 label과 hypothesis값(feature값이 들어오기 때문에 오류를 얻기 위해 호출)을 통해 원하는 cost값을 구할수있다. 
    return cost
#학습을 위한 함수이다.
def grad(hypothesis, features, labels): #hypothesis와 label 이 나온다.
    with tf.GradientTape() as tape:
        loss_value = loss_fn(logistic_regression(features),features,labels)
#hypothesis와 label으로 loss값에다 실제값과 가설을 통해 나온 값을 비교한 loss값을 구할수있다.
    return tape.gradient(loss_value, [w.b]) #gradient를 통해 실제 모들값을 계속 바꿔갈 수 있다.
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01) 
#GradientDescentOptimizer를 통해 실제 이동할 값(learning_rate)을 통한 값으로 optimizer선언한다.

EPOCHS = 1001

for step in range(EPOCHS): #함수를 EPOCHS만큼 돌린다.
    for features, labels  in tfe.Iterator(dataset): #dataset를 토대로 Iterator를 돌려서 실제 x값과 y값을 넣어가면서 모델이 만들어진다.
        grads = grad(logistic_regression(features), features, labels) #x값과 y값이 만들어진것을 집어넣어 grad값을 나오게 한다.
        optimizer.apply_gradients(grads_and_vars=zip(grads,[W,b])) 
        #나온 grad값을 Optimizer를 통해 minimizer하는 것을 구한다.
        #이 모델을 통해 w값과 b값 계속 업데이트를 통해 만들어지게된다. 
        #(cost값이 내려가게되어 원하는 최적의 값을 cost값을 줄여가면서 나타낼 수 있다.)
        if step % 100 == 0: #100번 마다
            print("Iter: {}, Loss: {:.4f}".format(step, loss_fn(logistic_regression(features),features,labels)))
        #원하는 Iter값과 Loss값이 줄어드는 것을 출력하기 위한 값이다.
           
def accuracy_fn(hypothesis, labels): #가설과 함수를 비교하기 위한 함수를 선언
    predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32) #x값을 넣었을때 hypothesis가 로지스틱펑션을 통해 나온 값이다.
    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.int32)) 
    #0.5를 통해 예측한 값이 나온다. 
    #실제 값과 예측하여 나온 값을 비교해서 이 값이 실제 맞는지 안맞는지 accuracy로 출력한다.
    return accuracy
    
test_acc = accuracy_fn(logistic_regression(x_test),y_test)
#test_acc자체를 x_test y_test를 넣어서 출력해낼 수 있다.
